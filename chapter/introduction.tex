\chapter{Introduction}
\section{Background}
Autonomous mobile robotic systems are emerging dramatically in recent years from
unmanned aerial vehicles (UAVs) to unmanned ground vehicles (UGVs), from self-driving
vehicle to indoor service robots. A large portion of industrial applications has
already achieved high-level autonomy through state-of-the-art techniques. 
For industrial robots, the operation environment is usually structurally constrained and fixed with various customized assisting modules such as using a pre-installed external visual marker system to assist localization and detection. 
Therefore, the industrial environment is barely changed with high stability, 
while for other demanding scopes of applications (e.g. autonomous driving, warehouse robots, and service robots), the operational surrounding changes significantly with high dynamics and uncertainties. This highlights the key focus on robustness and reliability under changing environments for the wide spread of autonomous mobile robots. In the development of Hercules\cite{liuhercules}, an autonomous vehicle targeting low-speed delivery, the stability under the dynamic environment is demonstrated and emphasized to enable safe autonomy. A picture of Hercules is shown in Fig\ref{fig:Hercules}.

Compared to a 3D laser scanner, also known as Light detection and ranging (LiDAR), 
visual sensors (i.e. cameras) are cheaper and able to provide rich
texture information, which is more suitable for high-level interactive tasks such
as mobile manipulation, and human-robot interaction. Cameras are also lightweight which enables massive and efficient deployment on all kinds of robotic platforms, e.g. on micro aerial robotics. While LiDARs are not suitable for their weight and size, as well as high drive power source requirements. Therefore, vision-based localization and mapping techniques are important which can achieve comparable performance with much lower hardware requirements.

This proposal focuses on the vision-based long-term navigation tasks, which require 
high robustness. During the task completion, the robot might encounter an environment with uncertainty, such as highly dynamic objects (humans, vehicles, and other operating robots), moderately dynamic objects (boxes, bins, and chairs), and static objects but could be changed or moved (fences, wall, and trees). This uncertainty of surrounding changes 
can lead to the failure of localization and inaccuracy of the map. Without a doubt, the localization of robots is the fundamental problem that other operations rely on.
Regarding map accuracy, the incremental changes and possible loss of localization make it important to maintain an up-to-date map. Moreover, the discovery of the human memory system shows the mechanism behind an incrementally and continuous learning system. The human memory system contains temporal memory as well as long-term memory. In 2022, BioSLAM\cite{yin2022bioslam} proposed a dual-memory system for place recognition system to maintain accuracy with various incremental new appearances. Current research seldom tackles this problem while mostly focusing on temporal navigation and assuming the environment to be static. Constrained by real-time computing resources as well as robustness, there is still a large gap toward long-term stable mobile navigation. Inspired by \cite{yin2022bioslam} and \cite{paton2016bridging}, both adopted the idea of building more than one map over the same changing environment, I proposed to carry out further research on strengthening the robustness of long-term perception system by leveraging multiple experiences and traversals over the same place. Specifically, from two perspectives, localization algorithm and map management mechanism.


\section{Related Work}
\subsection{Visual Teach-and-Repeat}
The teach-and-repeat navigation\cite{paul2010vtr,mcmanus2012visual} is one of the basic tasks in mobile robots, which enables a large number of applications such as autonomous delivery, transportation, inspection, and patrolling. As the name suggests, the system contains two phases. In the teaching phase, the robot usually travels along the desired trajectory by manual operation. And in the repeat phase, the autonomously follows the previously taught trajectory. This navigation system can be developed based on a variety of sensors including cameras\cite{paul2010vtr,mcmanus2012visual,dall2021fast,clement2017robust}, LiDARs\cite{sprunk2013lidar}, GPS\cite{li1998robot}, wheel encoders\cite{krajnik2018navigation} and sensor fusion\cite{nitsche2020visual}. In this proposal, the adopted sensors are limited to visual sensors, including monocular cameras, stereo cameras, and RGB-D cameras. Regarding the visual teach and repeat system, \cite{dall2021fast} achieves fast and robust teach and repeat with a topological map containing a sequence of ordered images. Furgale and Barfoot\cite{paul2010vtr} combine topological and metric map representation to build a manifold of overlapping locally consistent submaps. 
provides a state-of-the-art algorithm that enables navigation leveraging a topo-metric map. Paton\cite{paton2016bridging} further extends the system to record multiple repeated experiences with a Spatio-temporal pose graph to mitigate the incremental appearance changes. Most repeated tasks (e.g. inspection, delivery, and patrolling), are expected to be continuously repeated with a single teaching process, which is a typical scenario that the appearance changing would be challenging over a long time\cite{qian2022pocd}. Due to the extra uncertainty brought by the changes, it is difficult to achieve robust navigation with only an initial static map. Furthermore, this uncertainty in observation is also vital in various navigation tasks, such as a self-driving system that makes use of a high-definition map to implement localization and path planning. The changes in road lanes can't be ignored. To this end, this proposal was originally inspired by the observation of the teach-and-repeat system and expected to extend the solution to general mobile robot systems. 
\subsection{Long-term Metric Localization}
Accurate localization is an essential problem for autonomous robot operations. Vision-based localization is susceptible to appearance changes primarily due to the variation of spare image descriptors (e.g. SURF\cite{bay2008speeded}) under changing environments. An intuitive solution is to design an appearance invariant local feature. Learning-based methods have achieved impressive performance\cite{detone2018superpoint,revaud2019r2d2,sun2021robust,gridseth2021keeping} over lighting changes of day and night, weather, and seasons compared to handcraft features\cite{rublee2011orb,bay2008speeded}. In the meantime, visual place recognition (VPR) techniques have been developed for the image retrieval task, where the query image location is estimated using the localizations of the most visually similar images obtained from an image database. This task plays an important role in loop closure detection and re-localization problem. Convolutional Neural Network (CNN) based methods, led by NetVLAD\cite{arandjelovic2016netvlad} achieve recognition under large-scale, extreme viewpoint and appearance variation conditions. VPR provides a global topological localization, while in most navigation tasks, the six-degree-of-freedom (6DoF) metric localization is required to perform complex tasks. The state-of-art metric localization methods combine the VPR with sparse feature matching\cite{sarlin2019coarse,germain2019sparse,sarlin20superglue}, which is known as hierarchical localization. This localization framework leads the state-of-the-art performance on the long-term visual localization benchmark. As the deep learning techniques enable semantics inference, which promotes the research on leveraging the semantic property of the scene to enhance the localization\cite{larsson2019fine,stenborg2018long,taira2019right,yu2022accurate,7989305}. These works proposed to deal with long-term visual localization based on the semantics of the scene that appears to be invariant. This is true in most scenarios where the appearance changes are induced by the lighting variation, different weather conditions, and seasons. However, the changes caused by the movement of the object are not issued or evaluated in these works. In extreme situations, the movement of large-scale objects (e.g. buildings, fences, and trees) may cause the occlusion or newly appeared scenes. Long-term metric visual localization remains a challenge under dynamic conditions which has not been investigated thoroughly.

\subsection{Mapping Under Dynamic Scenes}
Accurate localization serves as the essential preliminary for globally consistent maps. There are different map representations or data structures designed for different purposes. Kimera\cite{Rosinol20icra-Kimera} provides a comprehensive collection of layered map representations, which reconstruct the environment from bottom to top. From the bottom of the metric point cloud layer to the top topological layer, different layers serve different robotic applications. Despite the various map representations constructed by Kimera, it does not highlight the uncertainty caused by dynamic objects. Within an environment, the objects can be classified as dynamic objects, semi-static objects, and static objects.
\cite{kim2020remove} proposed direct dynamic object removal on the point cloud, and further extend to a modular SLAM framework\cite{9811916}. This algorithm is suitable for continuously moving objects within the field of view, while it could not remove the objects that are moving under constant speed with respect to the sensor's ego-motion. Works have been done on dealing with dynamic objects and building dense volumetric maps based on truncated signed distance functions (TSDFs) \cite{schmid2022panoptic,grinvald2021tsdf,fehr2017tsdf,oleynikova2017voxblox,qian2022pocd}. POCD\cite{qian2022pocd} and Panoptic Mapping\cite{schmid2022panoptic}put focus on the indoor semi-static environments. For example, the furniture movement, warehouse goods movement, and parked cars movement in the parking lot. They achieve improvement over map maintenance, aiming to capture the environmental changes accurately and efficiently with the assumption of perfect localization is already provided. The tests on mapping under inaccurate or corrupted pose estimation have not been done to demonstrate the robustness of such a mapping system. Visual localization under changing environments remains an open problem, especially in the background of teach-and-repeat tasks. The map update system depends on the successful completion of repeated traversals. It is also more reasonable and practical to discuss the mapping under dynamic scenes combined with uncertainty localization primitive conditions.