\chapter{Preliminary Work}
In this chapter, preliminary works toward the proposed directions have been conducted which also serve as the root of motivation and inspiration.

\section{Localization and Mapping}
In this work, I am trying to build an indoor mapping and localization system for a quadruped robot to enable navigation tasks. For the mapping part, LiDAR odometry and mapping (LOAM, A-LOAM\cite{zhang2014loam}) are used during the localization part. In order to implement obstacle avoidance and path planning, an occupancy map is implemented online. However, when an object exists in an occupancy map for a long time, even after it disappears, the occupancy grid occupied by it won't be released. This is due to its bayesian update nature which brings map update to my attention. Part of the result is shown in Fig\ref{fig:quadruped} and Fig \ref{localizationeva}.
\section{Semantic Segmentation}
In this project, I got hands-on experience in training a semantic segmentation network from scratch and via fine-tuning, which achieves the segmentation tasks on a private and small-scale autonomous driving dataset with self-defined semantic classes. This serves as an evaluation of the state-of-the-art segmentation networks' generalization and domain adaptation abilities. In this project, DeepLabV3\cite{chen2017rethinking}, DeepLabV3+\cite{chen2018encoder}, MobileNetV3\cite{howard2019searching}, and BiSeNetV2\cite{yu2021bisenet} have been tested with results in Fig\ref{cross-evaluation} .
\section{2D Object Detection}
In this project, a relationship-oriented perception pipeline for accomplishing daily manipulation tasks is implemented. 2D object detection and semantic relationship inference are combined and explored to achieve object-level manipulation tasks. This builds up my experience in 2D object detection and inspired me to further research the semantic relationship between objects within a certain scene to construct high-level constraints over the consistency of map semantics. Partial results are shown in Fig\ref{fig:pipeline_relationship}. This work has been accepted by the 2022IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS2022) with more information can be referred to \url{https://www.youtube.com/watch?v=48rpq9SoPYo}.
\section{Visual Place Recognition}
Visual place recognition (VPR) in condition-varying environments is
still an open problem. In this paper\cite{ye2022condition}, we propose to use a convolutional autoencoder (CAE) to tackle this problem. We employ a high-level layer of a pre-trained CNN to generate features and train a CAE to map the features to a low-dimensional space to improve the condition invariance property of the descriptor and reduce its dimension at the same time (detailed approach shown in Fig\ref{fig:approach_VPR}). We verify our method in three challenging datasets involving significant illumination changes, and our method is shown to be superior to the state-of-the-art. The code is publicly available in \url{https://github.com/MedlarTea/CAE-VPR}.
\section{Multi-Sensor Fusion Dataset}
This paper\cite{jiao2022fusionportable} proposes the FusionPortable benchmark, a novel multi-sensor dataset with a diverse set of sequences for mobile robots, shown in Fig . This is a great fit for the collection of real-world long-term datasets mentioned in Section \ref{dataset}. We first advance a portable and versatile multi-sensor suite that offers rich sensory information: 10Hz LiDAR point clouds, 20Hz stereo frame images, high-rate and asynchronous events from stereo event cameras, 200Hz acceleration and angular velocity readings from a tactical grade IMU, and 10Hz GPS signal outdoors. Sensors are already temporally synchronized in hardware. This device is lightweight, self-contained, and has plug-and-play support for mobile robots. Second, we construct a dataset by collecting 18 sequences that cover a variety of environments on the campus by exploiting multiple platforms for data collection. 


